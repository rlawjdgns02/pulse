import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

print("=== SVM Multi-Kernel Hyperparameter Grid Search ===")

# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
X = pd.read_csv('features_left.csv')
y = pd.read_csv('labels_left.csv').astype(int)
y_flat = np.ravel(y)

print(f"ë°ì´í„° í¬ê¸°: {X.shape}")
print(f"ë ˆì´ë¸” ë¶„í¬: {np.bincount(y_flat)}")

# 2. Train/Test ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    X, y_flat, test_size=0.2, random_state=42, stratify=y_flat
)

# 3. ìŠ¤ì¼€ì¼ë§
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. ì»¤ë„ë³„ íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜
param_grids = [
    # Linear kernel (gamma ë¶ˆí•„ìš”)
    {
        'kernel': ['linear'], 
        'C': [0.1, 1, 10, 100, 1000]
    },
    
    # RBF kernel
    {
        'kernel': ['rbf'], 
        'C': [0.1, 1, 10, 100, 1000], 
        'gamma': ['scale', 'auto', 1, 0.1, 0.01, 0.001, 0.0001]
    },
    
    # Polynomial kernel
    {
        'kernel': ['poly'], 
        'C': [0.1, 1, 10, 100], 
        'gamma': ['scale', 'auto', 1, 0.1, 0.01],
        'degree': [2, 3, 4]
    },
    
    # Sigmoid kernel
    {
        'kernel': ['sigmoid'], 
        'C': [0.1, 1, 10, 100], 
        'gamma': ['scale', 'auto', 1, 0.1, 0.01, 0.001]
    }
]

# 5. í‰ê°€ ì§€í‘œ ì„¤ì •
scoring = {'accuracy': 'accuracy', 'f1_macro': 'f1_macro', 'precision_macro': 'precision_macro'}

# 6. GridSearchCV ì‹¤í–‰
print("\n--- Grid Search ì‹¤í–‰ ì¤‘ ---")
grid_search = GridSearchCV(
    SVC(random_state=42), 
    param_grids, 
    cv=5, 
    scoring=scoring, 
    refit='f1_macro',  # f1_macroë¡œ ìµœì  ëª¨ë¸ ì„ íƒ
    n_jobs=-1, 
    verbose=1
)

grid_search.fit(X_train_scaled, y_train)

# 7. ê²°ê³¼ ë¶„ì„
print("\n--- Grid Search ê²°ê³¼ ---")
print(f"ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
print(f"ìµœê³  êµì°¨ ê²€ì¦ F1-Score: {grid_search.best_score_:.4f}")

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ìµœì¢… ì„±ëŠ¥ í‰ê°€
best_model = grid_search.best_estimator_
test_accuracy = best_model.score(X_test_scaled, y_test)
y_pred = best_model.predict(X_test_scaled)

print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •í™•ë„: {test_accuracy:.4f}")

# 8. ì»¤ë„ë³„ ìµœê³  ì„±ëŠ¥ ë¹„êµ
results_df = pd.DataFrame(grid_search.cv_results_)

print("\n--- ì»¤ë„ë³„ ìµœê³  ì„±ëŠ¥ ë¹„êµ ---")
kernel_performance = {}

for kernel in ['linear', 'rbf', 'poly', 'sigmoid']:
    kernel_results = results_df[results_df['param_kernel'] == kernel]
    if len(kernel_results) > 0:
        best_idx = kernel_results['mean_test_f1_macro'].idxmax()
        best_result = results_df.loc[best_idx]
        
        kernel_performance[kernel] = {
            'f1_score': best_result['mean_test_f1_macro'],
            'accuracy': best_result['mean_test_accuracy'],
            'precision': best_result['mean_test_precision_macro'],
            'params': {k.replace('param_', ''): v for k, v in best_result.items() 
                      if k.startswith('param_')}
        }
        
        print(f"\n{kernel.upper()} ì»¤ë„:")
        print(f"  F1-Score: {best_result['mean_test_f1_macro']:.4f}")
        print(f"  ì •í™•ë„: {best_result['mean_test_accuracy']:.4f}")
        print(f"  ì •ë°€ë„: {best_result['mean_test_precision_macro']:.4f}")
        print(f"  íŒŒë¼ë¯¸í„°: {kernel_performance[kernel]['params']}")

# 9. ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ
print("\n--- í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¥˜ ë³´ê³ ì„œ ---")
print(classification_report(y_test, y_pred))

# 10. í˜¼ë™ í–‰ë ¬
print("\n--- í˜¼ë™ í–‰ë ¬ ---")
cm = confusion_matrix(y_test, y_pred)
print(cm)

# 11. ì‹œê°í™” í•¨ìˆ˜ë“¤
def plot_kernel_comparison(kernel_performance):
    """ì»¤ë„ë³„ ì„±ëŠ¥ ë¹„êµ ë§‰ëŒ€ê·¸ë˜í”„"""
    kernels = list(kernel_performance.keys())
    f1_scores = [kernel_performance[k]['f1_score'] for k in kernels]
    accuracies = [kernel_performance[k]['accuracy'] for k in kernels]
    precisions = [kernel_performance[k]['precision'] for k in kernels]
    
    x = np.arange(len(kernels))
    width = 0.25
    
    fig, ax = plt.subplots(figsize=(12, 6))
    
    ax.bar(x - width, f1_scores, width, label='F1-Score', alpha=0.8)
    ax.bar(x, accuracies, width, label='Accuracy', alpha=0.8)
    ax.bar(x + width, precisions, width, label='Precision', alpha=0.8)
    
    ax.set_xlabel('ì»¤ë„ ìœ í˜•')
    ax.set_ylabel('ì„±ëŠ¥ ì ìˆ˜')
    ax.set_title('SVM ì»¤ë„ë³„ ì„±ëŠ¥ ë¹„êµ')
    ax.set_xticks(x)
    ax.set_xticklabels(kernels)
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # ìˆ˜ì¹˜ í‘œì‹œ
    for i, (f1, acc, prec) in enumerate(zip(f1_scores, accuracies, precisions)):
        ax.text(i - width, f1 + 0.01, f'{f1:.3f}', ha='center', va='bottom')
        ax.text(i, acc + 0.01, f'{acc:.3f}', ha='center', va='bottom')
        ax.text(i + width, prec + 0.01, f'{prec:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig('svm_kernel_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

def plot_confusion_matrix(cm, classes=['í•œì²´ì§ˆ', 'ì—´ì²´ì§ˆ']):
    """í˜¼ë™ í–‰ë ¬ íˆíŠ¸ë§µ"""
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=classes, yticklabels=classes)
    plt.xlabel('ì˜ˆì¸¡ê°’')
    plt.ylabel('ì‹¤ì œê°’')
    plt.title('í˜¼ë™ í–‰ë ¬')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()

def plot_parameter_heatmap_by_kernel(results_df, kernel):
    """íŠ¹ì • ì»¤ë„ì˜ íŒŒë¼ë¯¸í„°ë³„ ì„±ëŠ¥ íˆíŠ¸ë§µ"""
    kernel_results = results_df[results_df['param_kernel'] == kernel].copy()
    
    if len(kernel_results) == 0:
        print(f"{kernel} ì»¤ë„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    try:
        if kernel == 'linear':
            # LinearëŠ” Cë§Œ ìˆìŒ
            plt.figure(figsize=(10, 4))
            c_values = sorted(kernel_results['param_C'].unique())
            f1_scores = []
            
            for c in c_values:
                score = kernel_results[kernel_results['param_C'] == c]['mean_test_f1_macro'].iloc[0]
                f1_scores.append(score)
            
            plt.bar(range(len(c_values)), f1_scores, alpha=0.7, color='skyblue')
            plt.xlabel('C ê°’')
            plt.ylabel('F1-Score')
            plt.title(f'{kernel.upper()} ì»¤ë„ íŒŒë¼ë¯¸í„°ë³„ ì„±ëŠ¥')
            plt.xticks(range(len(c_values)), [str(c) for c in c_values])
            
            # ìˆ˜ì¹˜ í‘œì‹œ
            for i, score in enumerate(f1_scores):
                plt.text(i, score + 0.005, f'{score:.3f}', ha='center', va='bottom')
            
            plt.grid(True, alpha=0.3)
            
        elif kernel in ['rbf', 'sigmoid']:
            # Cì™€ gammaê°€ ìˆëŠ” ì»¤ë„
            pivot_table = kernel_results.pivot_table(
                values='mean_test_f1_macro',
                index='param_gamma',
                columns='param_C',
                aggfunc='mean'
            )
            
            # gamma ê°’ ì •ë ¬ ë¬¸ì œ í•´ê²°: ë¬¸ìì—´ê³¼ ìˆ«ì ë¶„ë¦¬í•´ì„œ ì²˜ë¦¬
            gamma_order = []
            string_gammas = []
            numeric_gammas = []
            
            for gamma in pivot_table.index:
                if isinstance(gamma, str):
                    string_gammas.append(gamma)
                else:
                    numeric_gammas.append(gamma)
            
            # ë¬¸ìì—´ gammaë¥¼ ì•ì—, ìˆ«ì gammaë¥¼ í° ê°’ë¶€í„° ì •ë ¬
            string_gammas.sort()
            numeric_gammas.sort(reverse=True)
            gamma_order = string_gammas + numeric_gammas
            
            # ì •ë ¬ëœ ìˆœì„œë¡œ ì¸ë±ìŠ¤ ì¬ë°°ì—´
            pivot_table = pivot_table.reindex(gamma_order)
            
            plt.figure(figsize=(12, 8))
            sns.heatmap(pivot_table, annot=True, fmt='.4f', cmap='viridis',
                       cbar_kws={'label': 'F1-Score'})
            plt.xlabel('C ê°’')
            plt.ylabel('Gamma ê°’')
            plt.title(f'{kernel.upper()} ì»¤ë„ í•˜ì´í¼íŒŒë¼ë¯¸í„° íˆíŠ¸ë§µ')
            
        elif kernel == 'poly':
            # Polynomialì€ 3ì°¨ì›ì´ë¯€ë¡œ degreeë³„ë¡œ ë¶„ë¦¬í•´ì„œ í‘œì‹œ
            degrees = sorted(kernel_results['param_degree'].unique())
            
            fig, axes = plt.subplots(1, len(degrees), figsize=(6*len(degrees), 5))
            if len(degrees) == 1:
                axes = [axes]
            
            for i, degree in enumerate(degrees):
                degree_results = kernel_results[kernel_results['param_degree'] == degree]
                pivot_table = degree_results.pivot_table(
                    values='mean_test_f1_macro',
                    index='param_gamma',
                    columns='param_C',
                    aggfunc='mean'
                )
                
                # gamma ì •ë ¬ ë¬¸ì œ í•´ê²°
                gamma_order = []
                string_gammas = []
                numeric_gammas = []
                
                for gamma in pivot_table.index:
                    if isinstance(gamma, str):
                        string_gammas.append(gamma)
                    else:
                        numeric_gammas.append(gamma)
                
                string_gammas.sort()
                numeric_gammas.sort(reverse=True)
                gamma_order = string_gammas + numeric_gammas
                pivot_table = pivot_table.reindex(gamma_order)
                
                sns.heatmap(pivot_table, annot=True, fmt='.4f', cmap='viridis',
                           ax=axes[i], cbar_kws={'label': 'F1-Score'})
                axes[i].set_title(f'Degree = {int(degree)}')
                axes[i].set_xlabel('C ê°’')
                axes[i].set_ylabel('Gamma ê°’')
            
            plt.suptitle(f'{kernel.upper()} ì»¤ë„ í•˜ì´í¼íŒŒë¼ë¯¸í„° íˆíŠ¸ë§µ', fontsize=16)
        
        plt.tight_layout()
        plt.savefig(f'svm_{kernel}_heatmap.png', dpi=300, bbox_inches='tight')
        plt.show()
        
    except Exception as e:
        print(f"{kernel} ì»¤ë„ íˆíŠ¸ë§µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        print("íˆíŠ¸ë§µ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

# 12. ì‹œê°í™” ì‹¤í–‰
print("\n--- ì‹œê°í™” ìƒì„± ì¤‘ ---")

# ì»¤ë„ ë¹„êµ ì°¨íŠ¸
plot_kernel_comparison(kernel_performance)

# í˜¼ë™ í–‰ë ¬
plot_confusion_matrix(cm)

# ê° ì»¤ë„ë³„ íŒŒë¼ë¯¸í„° íˆíŠ¸ë§µ
for kernel in ['linear', 'rbf', 'sigmoid']:
    plot_parameter_heatmap_by_kernel(results_df, kernel)

# 13. ìµœì¢… ì¶”ì²œ
print("\n--- ìµœì¢… ì¶”ì²œ ---")
best_kernel = grid_search.best_params_['kernel']
print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ì»¤ë„: {best_kernel.upper()}")
print(f"ğŸ¯ ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
print(f"ğŸ“Š CV F1-Score: {grid_search.best_score_:.4f}")
print(f"ğŸ“ˆ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.4f}")

# ì„±ëŠ¥ ìˆœìœ„
performance_ranking = sorted(kernel_performance.items(), 
                           key=lambda x: x[1]['f1_score'], reverse=True)
print(f"\nì»¤ë„ ì„±ëŠ¥ ìˆœìœ„:")
for i, (kernel, perf) in enumerate(performance_ranking, 1):
    print(f"  {i}. {kernel.upper()}: F1={perf['f1_score']:.4f}")

print(f"\nëª¨ë“  ê·¸ë˜í”„ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")